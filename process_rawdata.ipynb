{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file pre-process the raw data in the following aspect:\n",
    "1. Filter the raw data and eliminate items with ratings less than 5 and users who has rated less than 5 movies.\n",
    "2. Holdout ratings: Split the dataset into training/testing set in proportion of approximately 80:20\n",
    "3. Split training set into three user group by their taste distribution. G1 user group represent blockbuster-focused user, G2 user group represet the Diverse taste user and G3 user group represent the niche-focused user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(tp, id):\n",
    "    # compute the frequency of given id(users/items)\n",
    "    playcount_groupbyid = tp[[id]].groupby(id, as_index=True)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_triplets(tp, min_uc=5, min_sc=0):\n",
    "    # Only keep the triplets for items which were clicked on by at least min_sc users. \n",
    "    if min_sc > 0:\n",
    "        # find the number of times of occurance of each movie id：\n",
    "        # itemcount contains movie ids and corresponding counts\n",
    "        itemcount = get_count(tp, 'movieId')\n",
    "        # select ratings of movies which occure more than min_sc times\n",
    "        tp = tp[tp['movieId'].isin(itemcount.index[itemcount>= min_sc])]\n",
    "    \n",
    "    # Only keep the triplets for users who clicked on at least min_uc items\n",
    "    # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
    "    if min_uc > 0:\n",
    "        usercount = get_count(tp, 'userId')\n",
    "        # select ratings of users who has rated more than min_uc items\n",
    "        tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
    "    \n",
    "    # Update both usercount and itemcount after filtering\n",
    "    usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId') \n",
    "    return tp, usercount, itemcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000209 entries, 0 to 1000208\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count    Dtype\n",
      "---  ------     --------------    -----\n",
      " 0   userId     1000209 non-null  int64\n",
      " 1   movieId    1000209 non-null  int64\n",
      " 2   rating     1000209 non-null  int64\n",
      " 3   timestamp  1000209 non-null  int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 30.5 MB\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "DATA_DIR = 'raw_data/ml-1m/'\n",
    "raw_data = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'), header=0)\n",
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering, there are 1000209 watching events from 6040 users and 3706 movies (sparsity: 4.468%)\n"
     ]
    }
   ],
   "source": [
    "raw_data, user_activity, item_popularity = filter_triplets(raw_data)\n",
    "sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
    "\n",
    "print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" % \n",
    "      (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_proportion(data, test_prop=0.2, randomSeed=98765):\n",
    "    data_grouped_by_user = data.groupby('userId')\n",
    "    tr_list, te_list = list(), list()\n",
    "\n",
    "    np.random.seed(randomSeed)\n",
    "\n",
    "    for i, (_, group) in enumerate(data_grouped_by_user):\n",
    "        n_items_u = len(group)\n",
    "\n",
    "        if n_items_u >= 5:\n",
    "            idx = np.zeros(n_items_u, dtype='bool')\n",
    "            idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
    "\n",
    "            tr_list.append(group[np.logical_not(idx)])\n",
    "            te_list.append(group[idx])\n",
    "        else:\n",
    "            tr_list.append(group)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"%d users sampled\" % i)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    data_tr = pd.concat(tr_list)\n",
    "    data_te = pd.concat(te_list)\n",
    "    \n",
    "    return data_tr, data_te"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 users sampled\n",
      "1000 users sampled\n",
      "2000 users sampled\n",
      "3000 users sampled\n",
      "4000 users sampled\n",
      "5000 users sampled\n",
      "6000 users sampled\n",
      "Training set contains: 80.24% ratings,\n",
      "Test set contains: 19.76% ratings\n"
     ]
    }
   ],
   "source": [
    "# split dataset，train：test = 80:20\n",
    "data_train, data_test = split_train_test_proportion(raw_data)\n",
    "print('Training set contains: {0:.2f}% ratings,\\nTest set contains: {1:.2f}% ratings'.format(100*data_train.shape[0]/raw_data.shape[0], 100*data_test.shape[0]/raw_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove timestamps in the saved files\n",
    "data_train = data_train.drop(['timestamp'],axis=1)\n",
    "data_test = data_test.drop(['timestamp'],axis=1)\n",
    "data_train.to_csv('processed_data/data_train.csv',header=False, index=False)\n",
    "data_test.to_csv('processed_data/data_test.csv',header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split user groups based on user's taste distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to compute the popularity of items. Since the popularity is defined as that the more peole interact with one item, the more popular this item is, we used the count of ratings of each movie to represent its popularity. Moreover, we normalise this count to range [0,1] by dividing the number of counts of the most popular item.\n",
    "\n",
    "We affiliate all items into three groups:\n",
    "\n",
    "1. H items: items whose total number of ratings occupy 20% total number of ratings and ranked at top of the sorted list of movie id.\n",
    "2. M items: whose total number of ratings occupy the 60% total number of ratings and ranked after head items.\n",
    "3. T items: whose total number of ratings occupy the 20% total number of ratings and ranked at the bottom of the sorted list of movie id.\n",
    "\n",
    "After this affiliation, we create a new column to store the label of item groups for each ratings in the dataset. The column 'movies_pop' has values 1 or 2 or 3 which represnt H or M or T type of items respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sort all movie id by popularity(Descending)\n",
    "raw_data_MoiveId = raw_data.groupby('movieId', as_index=True)\n",
    "sorted_movieId = raw_data_MoiveId.size().sort_values(ascending=False)\n",
    "# \n",
    "H_threshold = raw_data.shape[0]*0.2 # Head items\n",
    "M_threshold = raw_data.shape[0]*0.8 # Long-tail items\n",
    "\n",
    "# find the end index of H items and M items in the sorted list\n",
    "sum_pop = 0\n",
    "for index in range(len(sorted_movieId)): \n",
    "    if np.sum(sorted_movieId[:index]) >= H_threshold: break\n",
    "H_endindex = index\n",
    "for index in range(len(sorted_movieId)): \n",
    "    if np.sum(sorted_movieId[:index]) >= M_threshold: break\n",
    "M_endindex = index\n",
    "\n",
    "# max normalise the popularity to range [0,1]\n",
    "sorted_movieId = sorted_movieId/max(sorted_movieId) \n",
    "# store the movie ids into different groups\n",
    "H_movieId = sorted_movieId[:H_endindex]\n",
    "M_movieId = sorted_movieId[H_endindex:M_endindex]\n",
    "T_movieId = sorted_movieId[M_endindex:]\n",
    "# create labels for each type of movies\n",
    "movies_pop = raw_data['movieId'].isin(H_movieId.index)*1+raw_data['movieId'].isin(M_movieId.index)*2+raw_data['movieId'].isin(T_movieId.index)*3\n",
    "raw_data['movies_pop'] = movies_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'processed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIR, 'VAE_CF/unique_movieId.txt'), 'w') as f:\n",
    "    for movieId in sorted_movieId.index:\n",
    "        f.write('%s\\n' % movieId)\n",
    "with open(os.path.join(OUTPUT_DIR, 'VAE_CF/unique_userId.txt'), 'w') as f:\n",
    "    for userId in user_activity.index:\n",
    "        f.write('%s\\n' % userId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTPUT_DIR, 'VAE_CF/MovieCategories.txt'), 'w') as f:\n",
    "    for movieId in H_movieId.index:\n",
    "        f.write('%s,%s\\n' % (movieId,'H'))\n",
    "    for movieId in M_movieId.index:\n",
    "        f.write('%s,%s\\n' % (movieId,'M'))\n",
    "    for movieId in T_movieId.index:\n",
    "        f.write('%s,%s\\n' % (movieId,'T'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute users taste distribution for all users\n",
    "user_taste_dist = pd.DataFrame(columns=['H_ratio','T_ratio'])\n",
    "for userId in raw_data['userId'].drop_duplicates():\n",
    "    user_subset = raw_data[raw_data['userId'] == userId]\n",
    "    total_views = user_subset.shape[0]\n",
    "    H_ratio = user_subset[user_subset['movies_pop'] == 1].shape[0]/total_views\n",
    "    T_ratio = user_subset[user_subset['movies_pop'] == 3].shape[0]/total_views \n",
    "    user_taste = pd.DataFrame({'H_ratio': H_ratio,'T_ratio':T_ratio},index = [userId])\n",
    "    user_taste_dist = user_taste_dist.append(user_taste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_taste_dist.to_csv(os.path.join(OUTPUT_DIR,'VAE_CF/user_taste_dist.csv'),header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_taste_dist = user_taste_dist.sort_values(by='H_ratio',axis=0, ascending=False)\n",
    "H_ratio_mean = np.mean(user_taste_dist['H_ratio'])*100\n",
    "T_ratio_mean = np.mean(user_taste_dist['T_ratio'])*100\n",
    "M_ratio_mean = 100 - H_ratio_mean - T_ratio_mean\n",
    "print('the overall taste distribution of all users: H:{0:.2f}%, M:{1:.2f}%, T:{2:.2f}%'.format(H_ratio_mean,M_ratio_mean,T_ratio_mean))\n",
    "# split user group. G1:G2:G3 = 2:6:2\n",
    "G1_user = user_taste_dist.iloc[:int(0.2*user_taste_dist.shape[0])] # Blockbuster-focused Users\n",
    "G2_user = user_taste_dist.iloc[int(0.2*user_taste_dist.shape[0]):int(0.8*user_taste_dist.shape[0])] # Diverse Taste Users\n",
    "G3_user = user_taste_dist.iloc[int(0.8*user_taste_dist.shape[0]):]# Niche-focused Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user group 1 average taste distribution: H:45.93%, M:46.93%, T:7.14%\n",
      "user group 2 average taste distribution: H:25.50%, M:59.97%, T:14.53%\n",
      "user group 3 average taste distribution: H:12.42%, M:58.52%, T:29.06%\n"
     ]
    }
   ],
   "source": [
    "# compute average taste distribution for each group\n",
    "i = 0\n",
    "for group in [G1_user,G2_user,G3_user]:\n",
    "    i += 1\n",
    "    H = np.mean(group['H_ratio'])*100\n",
    "    T = np.mean(group['T_ratio'])*100\n",
    "    M  = 100-H-T\n",
    "    print('user group {0:} average taste distribution: H:{1:.2f}%, M:{2:.2f}%, T:{3:.2f}%'.format(i,H,M,T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1_ratings_full = raw_data[raw_data['userId'].isin(G1_user.index)]\n",
    "G2_ratings_full = raw_data[raw_data['userId'].isin(G2_user.index)]\n",
    "G3_ratings_full = raw_data[raw_data['userId'].isin(G3_user.index)]\n",
    "G1_ratings_train = data_train[data_train['userId'].isin(G1_user.index)]\n",
    "G2_ratings_train = data_train[data_train['userId'].isin(G2_user.index)]\n",
    "G3_ratings_train = data_train[data_train['userId'].isin(G3_user.index)]\n",
    "G1_ratings_test = data_test[data_test['userId'].isin(G1_user.index)]\n",
    "G2_ratings_test = data_test[data_test['userId'].isin(G2_user.index)]\n",
    "G3_ratings_test = data_test[data_test['userId'].isin(G3_user.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sorted movieId and corresponding popularity\n",
    "sorted_movieId.to_csv('processed_data/sorted_movieId.csv',header=False, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the corresponding user group with their taste distribution\n",
    "G1_user.to_csv(os.path.join(OUTPUT_DIR,'group_data/G1_user.csv'),header=True, index=True)\n",
    "G2_user.to_csv(os.path.join(OUTPUT_DIR,'group_data/G2_user.csv'),header=True, index=True)\n",
    "G3_user.to_csv(os.path.join(OUTPUT_DIR,'group_data/G3_user.csv'),header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset：save the corrsponding rating of users from each user group\n",
    "G1_ratings_full.to_csv(os.path.join(OUTPUT_DIR,'group_data/G1_ratings_full.csv'),header=False, index=False)\n",
    "G2_ratings_full.to_csv(os.path.join(OUTPUT_DIR,'group_data/G2_ratings_full.csv'),header=False, index=False)\n",
    "G3_ratings_full.to_csv(os.path.join(OUTPUT_DIR,'group_data/G3_ratings_full.csv'),header=False, index=False)\n",
    "# training set：save the corrsponding rating of users from each user group\n",
    "G1_ratings_train.to_csv(os.path.join(OUTPUT_DIR,'group_data/G1_ratings_train.csv'),header=False, index=False)\n",
    "G2_ratings_train.to_csv(os.path.join(OUTPUT_DIR,'group_data/G2_ratings_train.csv'),header=False, index=False)\n",
    "G3_ratings_train.to_csv(os.path.join(OUTPUT_DIR,'group_data/G3_ratings_train.csv'),header=False, index=False)\n",
    "# test set：save the corrsponding rating of users from each user group\n",
    "G1_ratings_test.to_csv(os.path.join(OUTPUT_DIR,'group_data/G1_ratings_test.csv'),header=False, index=False)\n",
    "G2_ratings_test.to_csv(os.path.join(OUTPUT_DIR,'group_data/G2_ratings_test.csv'),header=False, index=False)\n",
    "G3_ratings_test.to_csv(os.path.join(OUTPUT_DIR,'group_data/G3_ratings_test.csv'),header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set：save the corrsponding rating of users from each user group\n",
    "G1_ratings_train = pd.read_csv(os.path.join(OUTPUT_DIR,'group_data/G1_ratings_train.csv'),header=None, names=['userId', 'movieId','rating'])\n",
    "G2_ratings_train = pd.read_csv(os.path.join(OUTPUT_DIR,'group_data/G2_ratings_train.csv'),header=None, names=['userId', 'movieId','rating'])\n",
    "G3_ratings_train = pd.read_csv(os.path.join(OUTPUT_DIR,'group_data/G3_ratings_train.csv'),header=None, names=['userId', 'movieId','rating'])\n",
    "# test set：save the corrsponding rating of users from each user group\n",
    "G1_ratings_test = pd.read_csv(os.path.join(OUTPUT_DIR,'group_data/G1_ratings_test.csv'),header=None, names=['userId', 'movieId','rating'])\n",
    "G2_ratings_test = pd.read_csv(os.path.join(OUTPUT_DIR,'group_data/G2_ratings_test.csv'),header=None, names=['userId', 'movieId','rating'])\n",
    "G3_ratings_test = pd.read_csv(os.path.join(OUTPUT_DIR,'group_data/G3_ratings_test.csv'),header=None, names=['userId', 'movieId','rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer data format for long-tail GAN training.\n",
    "Since this is how the 'long-tail_GAN/Dataset/ml-1m' are generated, which is the dataset with holdout ratings. Eventually, we use the dataset as same way of split as VAE_CF, so the re-format of the dataset with holdout users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv('long-tail_GAN/Dataset/ml_1m/item_counts.csv',header=['userId','tagId','rating'], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sid = pd.unique(raw_data['movieId'])\n",
    "unique_uid = user_activity.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long-tail_GAN/Dataset/ml_1m/item_list.txt', 'w') as f:\n",
    "    for sid in unique_sid:\n",
    "        f.write('%s\\n' % sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long-tail_GAN/Dataset/ml_1m/unique_item_id.txt', 'w') as f:\n",
    "    for (i, sid) in enumerate(unique_sid):\n",
    "        f.write('%s\\n' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long-tail_GAN/Dataset/ml_1m/item2id.txt', 'w') as f:\n",
    "    for (i, sid) in enumerate(unique_sid):\n",
    "        f.write('%s\\t%s\\n' % (sid,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long-tail_GAN/Dataset/ml_1m/profile2id.txt', 'w') as f:\n",
    "    for (i, uid) in enumerate(unique_uid):\n",
    "        f.write('%s\\t%s\\n' % (uid,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long-tail_GAN/Dataset/ml_1m/niche_items.txt', 'w') as f:\n",
    "    for sid in pd.unique(raw_data[movies_pop == 2]['movieId']):\n",
    "        f.write('%s\\n' % sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show2id： {movieId: index in unique_sid}\n",
    "show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid)) # sid是movieId, i是这个movieId在unique_sid的index\n",
    "# profile2id： {userId: index in unique_uid}\n",
    "profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid)) # pid是userId，i是这个userId在unique_uid的index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerize(tp):\n",
    "    uid = list(map(lambda x: profile2id[x], tp['userId']))# 这里在map外面加了list 后续pandas才能处理\n",
    "    sid = list(map(lambda x: show2id[x], tp['movieId']))\n",
    "    return pd.DataFrame(data={'uid': uid, 'sid': sid}, columns=['uid', 'sid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerize(raw_data).to_csv('long-tail_GAN/Dataset/ml_1m/train_GAN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerize(raw_data[movies_pop == 1]).to_csv('long-tail_GAN/Dataset/ml_1m/train_GAN_popular.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerize(raw_data[movies_pop == 2]).to_csv('long-tail_GAN/Dataset/ml_1m/train_GAN_niche.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
